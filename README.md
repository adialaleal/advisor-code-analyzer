# Advisor Code Analyzer Agent

Agente FastAPI que analisa trechos de cÃ³digo Python e sugere melhorias com base em boas prÃ¡ticas, persistindo histÃ³rico das anÃ¡lises em PostgreSQL, cacheando resultados em Redis e pronto para ser orquestrado via CrewAI.

> **Arquitetura SOLID**: A aplicaÃ§Ã£o foi refatorada seguindo os princÃ­pios SOLID para garantir cÃ³digo modular, extensÃ­vel e testÃ¡vel. Consulte a seÃ§Ã£o [Arquitetura](#arquitetura) para mais detalhes.

## SumÃ¡rio
- [Arquitetura](#arquitetura)
- [PrÃ©-requisitos](#prÃ©-requisitos)
- [ConfiguraÃ§Ã£o](#configuraÃ§Ã£o)
- [ExecuÃ§Ã£o](#execuÃ§Ã£o)
- [Endpoints](#endpoints)
- [Base de Dados](#base-de-dados)
- [IntegraÃ§Ã£o CrewAI](#integraÃ§Ã£o-crewai)
- [Escalabilidade e Observabilidade](#escalabilidade-e-observabilidade)
- [Extensibilidade](#extensibilidade)
- [Testes manuais sugeridos](#testes-manuais-sugeridos)

## Arquitetura

A aplicaÃ§Ã£o segue os princÃ­pios **SOLID** para garantir cÃ³digo modular, extensÃ­vel e manutenÃ­vel.

```
app/
â”œâ”€â”€ api/                 # Camada HTTP (routers, deps)
â”œâ”€â”€ config.py            # ConfiguraÃ§Ãµes via Pydantic Settings
â”œâ”€â”€ crewai_integration/  # AbstraÃ§Ã£o de LLM e agente CrewAI
â”œâ”€â”€ interfaces/          # Protocolos e contratos (SOLID DIP)
â”œâ”€â”€ main.py              # Factory FastAPI
â”œâ”€â”€ models/              # ORM + schemas Pydantic
â””â”€â”€ services/            # Regras de negÃ³cio (analisador, cache, DB)
    â”œâ”€â”€ analysis_rules/  # Regras de anÃ¡lise individuais (SRP)
    â”‚   â”œâ”€â”€ imports.py
    â”‚   â”œâ”€â”€ variables.py
    â”‚   â”œâ”€â”€ functions.py
    â”‚   â”œâ”€â”€ naming.py
    â”‚   â”œâ”€â”€ docstrings.py
    â”‚   â””â”€â”€ statements.py
    â”œâ”€â”€ cache/           # Backends de cache separados (SRP)
    â”‚   â””â”€â”€ backends.py
    â”œâ”€â”€ analysis_service.py  # OrquestraÃ§Ã£o de anÃ¡lise
    â”œâ”€â”€ cache_service.py     # Facade de cache
    â”œâ”€â”€ code_analyzer.py     # Analisador com regras injetadas
    â””â”€â”€ database_service.py  # ServiÃ§o de persistÃªncia
scripts/
â””â”€â”€ init_db.sql          # CriaÃ§Ã£o e Ã­ndices da tabela analysis_history
```

### PrincÃ­pios SOLID Aplicados

**Single Responsibility Principle (SRP):**
- Cada classe tem uma Ãºnica responsabilidade bem definida
- Regras de anÃ¡lise separadas (`ImportAnalysisRule`, `UnusedVariableRule`, etc.)
- Backends de cache isolados (`RedisCacheBackend`, `MemoryCacheBackend`)

**Open/Closed Principle (OCP):**
- Sistema extensÃ­vel sem modificar cÃ³digo existente
- Novas regras de anÃ¡lise podem ser adicionadas facilmente
- ModelProviderFactory permite registrar novos provedores dinamicamente

**Liskov Substitution Principle (LSP):**
- Backends de cache sÃ£o intercambiÃ¡veis via interface

**Interface Segregation Principle (ISP):**
- Interfaces especÃ­ficas para cada contrato (`ICacheService`, `ICodeAnalyzer`, etc.)
- Protocolos Python definem contratos claros

**Dependency Inversion Principle (DIP):**
- DependÃªncias injetadas via FastAPI Depends
- Alto nÃ­vel nÃ£o depende de baixo nÃ­vel; ambos dependem de abstraÃ§Ãµes
- `CodeAnalysisService` orquestra toda a lÃ³gica de anÃ¡lise

### Principais Componentes

- **CodeAnalysisService** (`app/services/analysis_service.py`): orquestra anÃ¡lise, cache e persistÃªncia
- **CodeAnalyzer** (`app/services/code_analyzer.py`): aplica regras de anÃ¡lise com AST; aceita regras injetadas
- **Analysis Rules** (`app/services/analysis_rules/`): regras separadas (imports, variÃ¡veis, funÃ§Ãµes, naming, docstrings, prints)
- **AnalysisHistoryService** (`app/services/database_service.py`): persiste resultados em PostgreSQL usando SQLAlchemy
- **CacheService** (`app/services/cache_service.py`): facade que gerencia Redis com fallback para cache em memÃ³ria
- **Cache Backends** (`app/services/cache/backends.py`): Redis e Memory backends isolados
- **ModelProviderFactory** (`app/crewai_integration/model_provider.py`): abstrai provedores de LLM (OpenAI, Gemini, Anthropic, Azure OpenAI)
- **AdvisorCrewIntegration** (`app/crewai_integration/agent.py`): expÃµe o agente para um workflow CrewAI

## PrÃ©-requisitos
- Python 3.11+
- PostgreSQL (ou container existente)
- Docker (para subir Redis via `docker-compose`)

## ConfiguraÃ§Ã£o
1. Crie e ajuste o arquivo `.env` a partir do exemplo:
   ```bash
   cp .env.example .env
   ```
   - `DATABASE_URL`: string SQLAlchemy para seu PostgreSQL.
   - `REDIS_URL`: instÃ¢ncia local ou remota do Redis.
   - `MODEL_PROVIDER`: `openai`, `gemini`, `anthropic` ou `azure_openai`.
   - `CREWAI_API_KEY`: chave reutilizada pelo provedor escolhido (considere variÃ¡veis especÃ­ficas conforme docs oficiais da CrewAI [link](https://docs.crewai.com)).
   - Opcional: `AZURE_OPENAI_ENDPOINT` e `AZURE_OPENAI_DEPLOYMENT` para uso no Azure.

2. Instale as dependÃªncias Python (idealmente em um ambiente virtual):
   ```bash
   pip install -r requirements.txt
   ```

3. Suba o Redis localmente:
   ```bash
   docker-compose up -d
   ```

4. Execute o script de criaÃ§Ã£o da tabela (ajuste host/porta se necessÃ¡rio):
   ```bash
   psql "$DATABASE_URL" -f scripts/init_db.sql
   ```

## ExecuÃ§Ã£o
Inicie a API com Uvicorn:
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

A aplicaÃ§Ã£o expÃµe os endpoints em `http://localhost:8000/api/v1`.

## Testes
- **Guia completo**: consulte `docs/testing.md` para prÃ©-requisitos, exemplos de execuÃ§Ã£o do `pytest` e roteiros de testes manuais (curl/Postman).
- **ExecuÃ§Ã£o rÃ¡pida**: `python3 -m pytest` valida regras do analisador, serviÃ§o de cache, camada de persistÃªncia e endpoints principais.
- **Coverage**: Todos os 8 testes passam, incluindo testes unitÃ¡rios para regras individuais de anÃ¡lise, backends de cache, e testes de integraÃ§Ã£o da API.

## Endpoints
### `POST /api/v1/analyze-code`
Analisa cÃ³digo Python usando anÃ¡lise estÃ¡tica (AST). Retorna sugestÃµes tÃ©cnicas baseadas em regras prÃ©-definidas.

**Corpo:**
```json
{
  "code": "def foo():\n    print('hello')\n",
  "language_version": "3.11"
}
```

**Resposta:**
```json
{
  "code_hash": "...sha256...",
  "suggestions": [
    {
      "rule_id": "print_statement",
      "message": "Considere utilizar logging em vez de print para saÃ­da em produÃ§Ã£o.",
      "severity": "info",
      "line": 2,
      "column": 4,
      "metadata": {}
    }
  ],
  "analysis_time_ms": 8,
  "cached": false
}
```

- Resultados repetidos sÃ£o retornados do cache Redis (campo `cached` = `true`).
- Toda anÃ¡lise Ã© registrada em `analysis_history`.

### `POST /api/v1/analyze-code-llm` ğŸ†•
Analisa cÃ³digo Python usando CrewAI com LLM para gerar relatÃ³rio priorizado e contextualizado. Simula a integraÃ§Ã£o do agente com a plataforma CrewAI conforme descrito no desafio tÃ©cnico.

**Corpo:**
```json
{
  "code": "def foo():\n    print('hello')\n    return 1",
  "language_version": "3.11"
}
```

**Resposta:**
```json
{
  "code_hash": "...sha256...",
  "raw_suggestions": [
    {
      "rule_id": "missing_docstring",
      "message": "FunÃ§Ã£o 'foo' deveria conter docstring.",
      "severity": "info",
      "line": 1,
      "column": null,
      "metadata": {}
    },
    {
      "rule_id": "print_statement",
      "message": "Considere utilizar logging em vez de print para saÃ­da em produÃ§Ã£o.",
      "severity": "info",
      "line": 2,
      "column": 4,
      "metadata": {}
    }
  ],
  "prioritized_report": "RelatÃ³rio do LLM priorizando e contextualizando as sugestÃµes...",
  "model_used": "gemini",
  "analysis_time_ms": 1250,
  "cached": false
}
```

**Como Funciona:**
1. O cÃ³digo Ã© analisado pelo `CodeAnalyzer` (anÃ¡lise estÃ¡tica).
2. As sugestÃµes sÃ£o enviadas para o LLM (Gemini, GPT, Claude, etc.) via CrewAI.
3. O LLM prioriza as sugestÃµes, adiciona contexto e justificativas.
4. Retorna tanto as sugestÃµes brutas quanto o relatÃ³rio gerado pelo LLM.

**Requisitos:**
- `CREWAI_API_KEY` configurado no `.env`.
- `MODEL_PROVIDER` definido (`openai`, `gemini`, `anthropic`, ou `azure_openai`).

**âš ï¸ Nota:** A integraÃ§Ã£o com CrewAI/LLM requer configuraÃ§Ã£o adicional conforme documentaÃ§Ã£o oficial do CrewAI. O endpoint `/api/v1/analyze-code` (anÃ¡lise estÃ¡tica) funciona sem dependÃªncias adicionais de LLM.

### `GET /api/v1/health`
Retorna status das dependÃªncias (`database`, `cache`, `model_provider`).

## Base de Dados
Tabela `analysis_history`:
- `id` (UUID, gerado automaticamente)
- `code_hash` (hash SHA-256 do snippet)
- `code_snippet` (texto bruto, opcional)
- `suggestions` (JSONB com lista de recomendaÃ§Ãµes)
- `analysis_time_ms`, `language_version`
- `created_at` (`TIMESTAMPTZ`, default `NOW()`)

Ãndices:
- BTREE em `created_at`
- BTREE em `code_hash`
- GIN em `suggestions`

RecomendaÃ§Ãµes adicionais documentadas:
- Particionamento por faixa de datas para alto volume.
- Ajuste de connection pooling (`pool_size`, `max_overflow`).
- ExtensÃ£o `pgcrypto` habilitada pelo script para gerar UUID.

## IntegraÃ§Ã£o CrewAI

### VisÃ£o Geral da Arquitetura

Este agente foi desenvolvido para ser orquestrado pela plataforma **CrewAI** conforme especificado no desafio tÃ©cnico. A arquitetura implementa duas camadas:

1. **AnÃ¡lise EstÃ¡tica** (`CodeAnalyzer`): Usa `ast.parse()` para aplicar regras prÃ©-definidas
2. **AnÃ¡lise com LLM** (`AdvisorCrewIntegration`): Usa CrewAI para priorizar e contextualizar as sugestÃµes

### Arquitetura de IntegraÃ§Ã£o

```
Usuario â†’ API Endpoint â†’ CrewAI Workflow
                          â†“
                    CodeAnalyzer (Tool)
                          â†“
                    LLM (Gemini/GPT/Claude)
                          â†“
                    RelatÃ³rio Priorizado
```

### Como Funciona

**Endpoint com LLM (`/api/v1/analyze-code-llm`):**
- O cÃ³digo Ã© primeiro analisado pelo `CodeAnalyzer` (anÃ¡lise estÃ¡tica)
- As sugestÃµes sÃ£o enviadas para o LLM via CrewAI
- O LLM prioriza as sugestÃµes, adiciona contexto e justificativas
- Retorna tanto as sugestÃµes brutas quanto o relatÃ³rio gerado pelo LLM

**Tool CrewAI (`analyze_python_code`):**
- `CodeAnalyzer` Ã© exposto como uma tool do CrewAI
- Pode ser reutilizado em outros workflows CrewAI
- MantÃ©m consistÃªncia com a API REST

### ConfiguraÃ§Ã£o

1. Selecione o provedor via `MODEL_PROVIDER` (`openai`, `gemini`, `anthropic`, ou `azure_openai`)
2. Defina `CREWAI_API_KEY` no `.env`
3. Utilize `AdvisorCrewIntegration` para criar o agente:
   ```python
   from app.config import get_settings
   from app.crewai_integration.agent import AdvisorCrewIntegration
   from app.services.code_analyzer import CodeAnalyzer

   settings = get_settings()
   integration = AdvisorCrewIntegration(settings, CodeAnalyzer())
   agent = integration.build_agent()
   workflow = integration.build_sample_workflow()
   crew = workflow["crew"]
   result = crew.kickoff(inputs={"code_snippet": "def foo():\n    print('hi')"})
   ```

### BenefÃ­cios da IntegraÃ§Ã£o

âœ… **AnÃ¡lise HÃ­brida**: Combina anÃ¡lise estÃ¡tica precisa com contextualizaÃ§Ã£o inteligente do LLM
âœ… **PriorizaÃ§Ã£o Inteligente**: O LLM identifica quais problemas sÃ£o mais crÃ­ticos
âœ… **Contexto Adicionado**: Explica o "porquÃª" de cada sugestÃ£o
âœ… **Flexibilidade**: Pode ser usado como API REST ou como tool no CrewAI
âœ… **Escalabilidade**: FÃ¡cil adicionar novos provedores de LLM via `ModelProviderFactory`

### DocumentaÃ§Ã£o Adicional

Consulte a documentaÃ§Ã£o oficial da CrewAI para conectar triggers e flows empresariais: [CrewAI Docs](https://docs.crewai.com)

## Escalabilidade e Observabilidade
- **Cache**: Redis com TTL de 1h. Fallback in-memory garante disponibilidade local.
- **Filas**: Utilize RabbitMQ ou Redis Streams para enviar anÃ¡lises volumosas a workers dedicados (ex.: Celery, RQ). O README inclui passos para evoluÃ§Ã£o futura.
- **Horizontal Scaling**: API stateless; basta replicar instÃ¢ncias atrÃ¡s de um load balancer. Configure sticky sessions apenas se necessÃ¡rio.
- **Banco de Dados**: IndexaÃ§Ã£o em `code_hash` e particionamento temporal reduz leituras pesadas; considerar compressÃ£o do campo `code_snippet` para histÃ³ricos antigos.
- **Observabilidade**: Healthcheck consolidado, logging estruturado (ajustÃ¡vel via `LOG_LEVEL`), espaÃ§o reservado para mÃ©tricas em `main.py` (lifespan). Integrar com Prometheus/OpenTelemetry em etapas posteriores.

## Extensibilidade

A arquitetura SOLID facilita a extensÃ£o do sistema:

### Adicionar Novas Regras de AnÃ¡lise

Crie uma nova regra estendendo `BaseAnalysisRule`:

```python
from app.services.analysis_rules.base import BaseAnalysisRule
from typing import Any, Dict, List
import ast

class MyCustomRule(BaseAnalysisRule):
    rule_id = "my_custom_rule"
    
    def _analyze(self, tree: ast.AST, suggestions: List[Dict[str, Any]]) -> None:
        # Sua lÃ³gica de anÃ¡lise aqui
        pass
```

Em seguida, registre a regra no `CodeAnalyzer`:

```python
from app.services.code_analyzer import CodeAnalyzer
from app.services.analysis_rules import ImportAnalysisRule, UnusedVariableRule
from my_module import MyCustomRule

rules = [
    ImportAnalysisRule(),
    UnusedVariableRule(),
    MyCustomRule(),  # Nova regra
]
analyzer = CodeAnalyzer(rules=rules)
```

### Adicionar Novo Provedor de LLM

Registre um novo provedor de modelo dinamicamente:

```python
from app.crewai_integration.model_provider import ModelProviderFactory, BaseModelProvider

class MyCustomProvider(BaseModelProvider):
    provider_name = "my_provider"
    
    def get_llm_config(self):
        return {"model": "my-model", "api_key": self.api_key}

ModelProviderFactory.register_provider("my_provider", MyCustomProvider)
```

### Adicionar Novo Backend de Cache

Crie um novo backend implementando `ICacheBackend`:

```python
from app.services.cache.backends import ICacheBackend
from typing import Any, Optional

class MyCacheBackend(ICacheBackend):
    def get(self, key: str) -> Optional[Any]:
        # Sua implementaÃ§Ã£o
        pass
    
    def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -> None:
        # Sua implementaÃ§Ã£o
        pass
    
    def is_available(self) -> bool:
        return True
```

Em seguida, use-o no `CacheService`:

```python
from app.services.cache_service import CacheService
from my_module import MyCacheBackend

my_backend = MyCacheBackend()
cache = CacheService("", primary_backend=my_backend)
```

## Testes manuais sugeridos
1. **Happy path**: enviar snippet vÃ¡lido e verificar gravaÃ§Ã£o na tabela.
2. **Cache hit**: repetir o mesmo snippet e checar `cached=true`.
3. **Erro de sintaxe**: garantir que mensagens de erro sejam retornadas.
4. **Healthcheck**: desligar Redis e verificar que o endpoint indica `cache=fallback`.
5. **CrewAI**: executar `build_sample_workflow()` e validar chamadas da tool.

---

> ReferÃªncia de documentaÃ§Ã£o: CrewAI Docs em [docs.crewai.com](https://docs.crewai.com)
