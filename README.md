# Advisor Code Analyzer Agent

Agente FastAPI que analisa trechos de cÃ³digo Python e sugere melhorias com base em boas prÃ¡ticas, persistindo histÃ³rico das anÃ¡lises em PostgreSQL, cacheando resultados em Redis e pronto para ser orquestrado via CrewAI.

## SumÃ¡rio
- [Arquitetura](#arquitetura)
- [PrÃ©-requisitos](#prÃ©-requisitos)
- [ConfiguraÃ§Ã£o](#configuraÃ§Ã£o)
- [ExecuÃ§Ã£o](#execuÃ§Ã£o)
- [Endpoints](#endpoints)
- [Base de Dados](#base-de-dados)
- [IntegraÃ§Ã£o CrewAI](#integraÃ§Ã£o-crewai)
- [Escalabilidade e Observabilidade](#escalabilidade-e-observabilidade)
- [Testes manuais sugeridos](#testes-manuais-sugeridos)

## Arquitetura
```
app/
â”œâ”€â”€ api/                 # Camada HTTP (routers, deps)
â”œâ”€â”€ config.py            # ConfiguraÃ§Ãµes via Pydantic Settings
â”œâ”€â”€ crewai_integration/  # AbstraÃ§Ã£o de LLM e agente CrewAI
â”œâ”€â”€ main.py              # Factory FastAPI
â”œâ”€â”€ models/              # ORM + schemas Pydantic
â””â”€â”€ services/            # Regras de negÃ³cio (analisador, cache, DB)
scripts/
â””â”€â”€ init_db.sql          # CriaÃ§Ã£o e Ã­ndices da tabela analysis_history
```

Principais componentes:
- **CodeAnalyzer** (`app/services/code_analyzer.py`): usa `ast` para aplicar regras como importaÃ§Ãµes/variÃ¡veis nÃ£o utilizadas, complexidade ciclomÃ¡tica, funÃ§Ãµes longas, docstrings e convenÃ§Ãµes PEP 8.
- **AnalysisHistoryService** (`app/services/database_service.py`): persiste resultados em PostgreSQL usando SQLAlchemy.
- **CacheService** (`app/services/cache_service.py`): tenta Redis; se indisponÃ­vel, faz fallback para cache em memÃ³ria com TTL.
- **ModelProviderFactory** (`app/crewai_integration/model_provider.py`): abstrai provedores de LLM (OpenAI, Gemini, Anthropic, Azure OpenAI) permitindo alternÃ¢ncia configurÃ¡vel.
- **AdvisorCrewIntegration** (`app/crewai_integration/agent.py`): exemplo de como expor o agente para um workflow CrewAI, incluindo tool que reutiliza o analisador.

## PrÃ©-requisitos
- Python 3.11+
- PostgreSQL (ou container existente)
- Docker (para subir Redis via `docker-compose`)

## ConfiguraÃ§Ã£o
1. Crie e ajuste o arquivo `.env` a partir do exemplo:
   ```bash
   cp .env.example .env
   ```
   - `DATABASE_URL`: string SQLAlchemy para seu PostgreSQL.
   - `REDIS_URL`: instÃ¢ncia local ou remota do Redis.
   - `MODEL_PROVIDER`: `openai`, `gemini`, `anthropic` ou `azure_openai`.
   - `CREWAI_API_KEY`: chave reutilizada pelo provedor escolhido (considere variÃ¡veis especÃ­ficas conforme docs oficiais da CrewAI [link](https://docs.crewai.com)).
   - Opcional: `AZURE_OPENAI_ENDPOINT` e `AZURE_OPENAI_DEPLOYMENT` para uso no Azure.

2. Instale as dependÃªncias Python (idealmente em um ambiente virtual):
   ```bash
   pip install -r requirements.txt
   ```

3. Suba o Redis localmente:
   ```bash
   docker-compose up -d
   ```

4. Execute o script de criaÃ§Ã£o da tabela (ajuste host/porta se necessÃ¡rio):
   ```bash
   psql "$DATABASE_URL" -f scripts/init_db.sql
   ```

## ExecuÃ§Ã£o
Inicie a API com Uvicorn:
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

A aplicaÃ§Ã£o expÃµe os endpoints em `http://localhost:8000/api/v1`.

## Testes
- Guia completo: consulte `docs/testing.md` para prÃ©-requisitos, exemplos de execuÃ§Ã£o do `pytest` e roteiros de testes manuais (curl/Postman).
- ExecuÃ§Ã£o rÃ¡pida: `python3 -m pytest` valida regras do analisador, serviÃ§o de cache, camada de persistÃªncia e endpoints principais.

## Endpoints
### `POST /api/v1/analyze-code`
Analisa cÃ³digo Python usando anÃ¡lise estÃ¡tica (AST). Retorna sugestÃµes tÃ©cnicas baseadas em regras prÃ©-definidas.

**Corpo:**
```json
{
  "code": "def foo():\n    print('hello')\n",
  "language_version": "3.11"
}
```

**Resposta:**
```json
{
  "code_hash": "...sha256...",
  "suggestions": [
    {
      "rule_id": "print_statement",
      "message": "Considere utilizar logging em vez de print para saÃ­da em produÃ§Ã£o.",
      "severity": "info",
      "line": 2,
      "column": 4,
      "metadata": {}
    }
  ],
  "analysis_time_ms": 8,
  "cached": false
}
```

- Resultados repetidos sÃ£o retornados do cache Redis (campo `cached` = `true`).
- Toda anÃ¡lise Ã© registrada em `analysis_history`.

### `POST /api/v1/analyze-code-llm` ğŸ†•
Analisa cÃ³digo Python usando CrewAI com LLM para gerar relatÃ³rio priorizado e contextualizado. Simula a integraÃ§Ã£o do agente com a plataforma CrewAI conforme descrito no desafio tÃ©cnico.

**Corpo:**
```json
{
  "code": "def foo():\n    print('hello')\n    return 1",
  "language_version": "3.11"
}
```

**Resposta:**
```json
{
  "code_hash": "...sha256...",
  "raw_suggestions": [
    {
      "rule_id": "missing_docstring",
      "message": "FunÃ§Ã£o 'foo' deveria conter docstring.",
      "severity": "info",
      "line": 1,
      "column": null,
      "metadata": {}
    },
    {
      "rule_id": "print_statement",
      "message": "Considere utilizar logging em vez de print para saÃ­da em produÃ§Ã£o.",
      "severity": "info",
      "line": 2,
      "column": 4,
      "metadata": {}
    }
  ],
  "prioritized_report": "RelatÃ³rio do LLM priorizando e contextualizando as sugestÃµes...",
  "model_used": "gemini",
  "analysis_time_ms": 1250,
  "cached": false
}
```

**Como Funciona:**
1. O cÃ³digo Ã© analisado pelo `CodeAnalyzer` (anÃ¡lise estÃ¡tica).
2. As sugestÃµes sÃ£o enviadas para o LLM (Gemini, GPT, Claude, etc.) via CrewAI.
3. O LLM prioriza as sugestÃµes, adiciona contexto e justificativas.
4. Retorna tanto as sugestÃµes brutas quanto o relatÃ³rio gerado pelo LLM.

**Requisitos:**
- `CREWAI_API_KEY` configurado no `.env`.
- `MODEL_PROVIDER` definido (`openai`, `gemini`, `anthropic`, ou `azure_openai`).

**âš ï¸ Nota:** A integraÃ§Ã£o com CrewAI/LLM requer configuraÃ§Ã£o adicional conforme documentaÃ§Ã£o oficial do CrewAI. O endpoint `/api/v1/analyze-code` (anÃ¡lise estÃ¡tica) funciona sem dependÃªncias adicionais de LLM.

### `GET /api/v1/health`
Retorna status das dependÃªncias (`database`, `cache`, `model_provider`).

## Base de Dados
Tabela `analysis_history`:
- `id` (UUID, default `gen_random_uuid()`)
- `code_hash` (hash SHA-256 do snippet)
- `code_snippet` (texto bruto, opcional)
- `suggestions` (JSONB com lista de recomendaÃ§Ãµes)
- `analysis_time_ms`, `language_version`
- `created_at` (`TIMESTAMPTZ`, default `NOW()`)

Ãndices:
- BTREE em `created_at`
- BTREE em `code_hash`
- GIN em `suggestions`

RecomendaÃ§Ãµes adicionais documentadas:
- Particionamento por faixa de datas para alto volume.
- Ajuste de connection pooling (`pool_size`, `max_overflow`).
- ExtensÃ£o `pgcrypto` habilitada pelo script para gerar UUID.

## IntegraÃ§Ã£o CrewAI

### VisÃ£o Geral da Arquitetura

Este agente foi desenvolvido para ser orquestrado pela plataforma **CrewAI** conforme especificado no desafio tÃ©cnico. A arquitetura implementa duas camadas:

1. **AnÃ¡lise EstÃ¡tica** (`CodeAnalyzer`): Usa `ast.parse()` para aplicar regras prÃ©-definidas
2. **AnÃ¡lise com LLM** (`AdvisorCrewIntegration`): Usa CrewAI para priorizar e contextualizar as sugestÃµes

### Arquitetura de IntegraÃ§Ã£o

```
Usuario â†’ API Endpoint â†’ CrewAI Workflow
                          â†“
                    CodeAnalyzer (Tool)
                          â†“
                    LLM (Gemini/GPT/Claude)
                          â†“
                    RelatÃ³rio Priorizado
```

### Como Funciona

**Endpoint com LLM (`/api/v1/analyze-code-llm`):**
- O cÃ³digo Ã© primeiro analisado pelo `CodeAnalyzer` (anÃ¡lise estÃ¡tica)
- As sugestÃµes sÃ£o enviadas para o LLM via CrewAI
- O LLM prioriza as sugestÃµes, adiciona contexto e justificativas
- Retorna tanto as sugestÃµes brutas quanto o relatÃ³rio gerado pelo LLM

**Tool CrewAI (`analyze_python_code`):**
- `CodeAnalyzer` Ã© exposto como uma tool do CrewAI
- Pode ser reutilizado em outros workflows CrewAI
- MantÃ©m consistÃªncia com a API REST

### ConfiguraÃ§Ã£o

1. Selecione o provedor via `MODEL_PROVIDER` (`openai`, `gemini`, `anthropic`, ou `azure_openai`)
2. Defina `CREWAI_API_KEY` no `.env`
3. Utilize `AdvisorCrewIntegration` para criar o agente:
   ```python
   from app.config import get_settings
   from app.crewai_integration.agent import AdvisorCrewIntegration
   from app.services.code_analyzer import CodeAnalyzer

   settings = get_settings()
   integration = AdvisorCrewIntegration(settings, CodeAnalyzer())
   agent = integration.build_agent()
   workflow = integration.build_sample_workflow()
   crew = workflow["crew"]
   result = crew.kickoff(inputs={"code_snippet": "def foo():\n    print('hi')"})
   ```

### BenefÃ­cios da IntegraÃ§Ã£o

âœ… **AnÃ¡lise HÃ­brida**: Combina anÃ¡lise estÃ¡tica precisa com contextualizaÃ§Ã£o inteligente do LLM
âœ… **PriorizaÃ§Ã£o Inteligente**: O LLM identifica quais problemas sÃ£o mais crÃ­ticos
âœ… **Contexto Adicionado**: Explica o "porquÃª" de cada sugestÃ£o
âœ… **Flexibilidade**: Pode ser usado como API REST ou como tool no CrewAI
âœ… **Escalabilidade**: FÃ¡cil adicionar novos provedores de LLM via `ModelProviderFactory`

### DocumentaÃ§Ã£o Adicional

Consulte a documentaÃ§Ã£o oficial da CrewAI para conectar triggers e flows empresariais: [CrewAI Docs](https://docs.crewai.com)

## Escalabilidade e Observabilidade
- **Cache**: Redis com TTL de 1h. Fallback in-memory garante disponibilidade local.
- **Filas**: Utilize RabbitMQ ou Redis Streams para enviar anÃ¡lises volumosas a workers dedicados (ex.: Celery, RQ). O README inclui passos para evoluÃ§Ã£o futura.
- **Horizontal Scaling**: API stateless; basta replicar instÃ¢ncias atrÃ¡s de um load balancer. Configure sticky sessions apenas se necessÃ¡rio.
- **Banco de Dados**: IndexaÃ§Ã£o em `code_hash` e particionamento temporal reduz leituras pesadas; considerar compressÃ£o do campo `code_snippet` para histÃ³ricos antigos.
- **Observabilidade**: Healthcheck consolidado, logging estruturado (ajustÃ¡vel via `LOG_LEVEL`), espaÃ§o reservado para mÃ©tricas em `main.py` (lifespan). Integrar com Prometheus/OpenTelemetry em etapas posteriores.

## Testes manuais sugeridos
1. **Happy path**: enviar snippet vÃ¡lido e verificar gravaÃ§Ã£o na tabela.
2. **Cache hit**: repetir o mesmo snippet e checar `cached=true`.
3. **Erro de sintaxe**: garantir que mensagens de erro sejam retornadas.
4. **Healthcheck**: desligar Redis e verificar que o endpoint indica `cache=fallback`.
5. **CrewAI**: executar `build_sample_workflow()` e validar chamadas da tool.

---

> ReferÃªncia de documentaÃ§Ã£o: CrewAI Docs em [docs.crewai.com](https://docs.crewai.com)
