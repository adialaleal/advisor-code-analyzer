# Advisor Code Analyzer Agent

Agente FastAPI que analisa trechos de c√≥digo Python e sugere melhorias com base em boas pr√°ticas, persistindo hist√≥rico das an√°lises em PostgreSQL, cacheando resultados em Redis e pronto para ser orquestrado via CrewAI.

## Sum√°rio
- [Arquitetura](#arquitetura)
- [Pr√©-requisitos](#pr√©-requisitos)
- [Configura√ß√£o](#configura√ß√£o)
- [Execu√ß√£o](#execu√ß√£o)
- [Endpoints](#endpoints)
- [Base de Dados](#base-de-dados)
- [Integra√ß√£o CrewAI](#integra√ß√£o-crewai)
- [Escalabilidade e Observabilidade](#escalabilidade-e-observabilidade)
- [Testes manuais sugeridos](#testes-manuais-sugeridos)

## Arquitetura
```
app/
‚îú‚îÄ‚îÄ api/                 # Camada HTTP (routers, deps)
‚îú‚îÄ‚îÄ config.py            # Configura√ß√µes via Pydantic Settings
‚îú‚îÄ‚îÄ crewai_integration/  # Abstra√ß√£o de LLM e agente CrewAI
‚îú‚îÄ‚îÄ main.py              # Factory FastAPI
‚îú‚îÄ‚îÄ models/              # ORM + schemas Pydantic
‚îî‚îÄ‚îÄ services/            # Regras de neg√≥cio (analisador, cache, DB)
scripts/
‚îî‚îÄ‚îÄ init_db.sql          # Cria√ß√£o e √≠ndices da tabela analysis_history
```

Principais componentes:
- **CodeAnalyzer** (`app/services/code_analyzer.py`): usa `ast` para aplicar regras como importa√ß√µes/vari√°veis n√£o utilizadas, complexidade ciclom√°tica, fun√ß√µes longas, docstrings e conven√ß√µes PEP 8.
- **AnalysisHistoryService** (`app/services/database_service.py`): persiste resultados em PostgreSQL usando SQLAlchemy.
- **CacheService** (`app/services/cache_service.py`): tenta Redis; se indispon√≠vel, faz fallback para cache em mem√≥ria com TTL.
- **ModelProviderFactory** (`app/crewai_integration/model_provider.py`): abstrai provedores de LLM (OpenAI, Gemini, Anthropic, Azure OpenAI) permitindo altern√¢ncia configur√°vel.
- **AdvisorCrewIntegration** (`app/crewai_integration/agent.py`): exemplo de como expor o agente para um workflow CrewAI, incluindo tool que reutiliza o analisador.

## Pr√©-requisitos
- Python 3.11+
- PostgreSQL (ou container existente)
- Docker (para subir Redis via `docker-compose`)

## Configura√ß√£o
1. Crie e ajuste o arquivo `.env` a partir do exemplo:
   ```bash
   cp .env.example .env
   ```
   - `DATABASE_URL`: string SQLAlchemy para seu PostgreSQL.
   - `REDIS_URL`: inst√¢ncia local ou remota do Redis.
   - `MODEL_PROVIDER`: `openai`, `gemini`, `anthropic` ou `azure_openai`.
   - `CREWAI_API_KEY`: chave reutilizada pelo provedor escolhido (considere vari√°veis espec√≠ficas conforme docs oficiais da CrewAI [link](https://docs.crewai.com)).
   - Opcional: `AZURE_OPENAI_ENDPOINT` e `AZURE_OPENAI_DEPLOYMENT` para uso no Azure.

2. Instale as depend√™ncias Python (idealmente em um ambiente virtual):
   ```bash
   pip install -r requirements.txt
   ```

3. Suba o Redis localmente:
   ```bash
   docker-compose up -d
   ```

4. Execute o script de cria√ß√£o da tabela (ajuste host/porta se necess√°rio):
   ```bash
   psql "$DATABASE_URL" -f scripts/init_db.sql
   ```

## Execu√ß√£o
Inicie a API com Uvicorn:
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

A aplica√ß√£o exp√µe os endpoints em `http://localhost:8000/api/v1`.

## Testes
- Guia completo: consulte `docs/testing.md` para pr√©-requisitos, exemplos de execu√ß√£o do `pytest` e roteiros de testes manuais (curl/Postman).
- Execu√ß√£o r√°pida: `python3 -m pytest` valida regras do analisador, servi√ßo de cache, camada de persist√™ncia e endpoints principais.

## Endpoints
### `POST /api/v1/analyze-code`
Analisa c√≥digo Python usando an√°lise est√°tica (AST). Retorna sugest√µes t√©cnicas baseadas em regras pr√©-definidas.

**Corpo:**
```json
{
  "code": "def foo():\n    print('hello')\n",
  "language_version": "3.11"
}
```

**Resposta:**
```json
{
  "code_hash": "...sha256...",
  "suggestions": [
    {
      "rule_id": "print_statement",
      "message": "Considere utilizar logging em vez de print para sa√≠da em produ√ß√£o.",
      "severity": "info",
      "line": 2,
      "column": 4,
      "metadata": {}
    }
  ],
  "analysis_time_ms": 8,
  "cached": false
}
```

- Resultados repetidos s√£o retornados do cache Redis (campo `cached` = `true`).
- Toda an√°lise √© registrada em `analysis_history`.

### `POST /api/v1/analyze-code-llm` üÜï
Analisa c√≥digo Python usando CrewAI com LLM para gerar relat√≥rio priorizado e contextualizado. Simula a integra√ß√£o do agente com a plataforma CrewAI conforme descrito no desafio t√©cnico.

**Corpo:**
```json
{
  "code": "def foo():\n    print('hello')\n    return 1",
  "language_version": "3.11"
}
```

**Resposta:**
```json
{
  "code_hash": "...sha256...",
  "raw_suggestions": [
    {
      "rule_id": "missing_docstring",
      "message": "Fun√ß√£o 'foo' deveria conter docstring.",
      "severity": "info",
      "line": 1,
      "column": null,
      "metadata": {}
    },
    {
      "rule_id": "print_statement",
      "message": "Considere utilizar logging em vez de print para sa√≠da em produ√ß√£o.",
      "severity": "info",
      "line": 2,
      "column": 4,
      "metadata": {}
    }
  ],
  "prioritized_report": "Relat√≥rio do LLM priorizando e contextualizando as sugest√µes...",
  "model_used": "gemini",
  "analysis_time_ms": 1250,
  "cached": false
}
```

**Como Funciona:**
1. O c√≥digo √© analisado pelo `CodeAnalyzer` (an√°lise est√°tica).
2. As sugest√µes s√£o enviadas para o LLM (Gemini, GPT, Claude, etc.) via CrewAI.
3. O LLM prioriza as sugest√µes, adiciona contexto e justificativas.
4. Retorna tanto as sugest√µes brutas quanto o relat√≥rio gerado pelo LLM.

**Requisitos:**
- `CREWAI_API_KEY` configurado no `.env`.
- `MODEL_PROVIDER` definido (`openai`, `gemini`, `anthropic`, ou `azure_openai`).

**‚ö†Ô∏è Nota:** A integra√ß√£o com CrewAI/LLM requer configura√ß√£o adicional conforme documenta√ß√£o oficial do CrewAI. O endpoint `/api/v1/analyze-code` (an√°lise est√°tica) funciona sem depend√™ncias adicionais de LLM.

### `GET /api/v1/health`
Retorna status das depend√™ncias (`database`, `cache`, `model_provider`).

## Base de Dados
Tabela `analysis_history`:
- `id` (UUID, default `gen_random_uuid()`)
- `code_hash` (hash SHA-256 do snippet)
- `code_snippet` (texto bruto, opcional)
- `suggestions` (JSONB com lista de recomenda√ß√µes)
- `analysis_time_ms`, `language_version`
- `created_at` (`TIMESTAMPTZ`, default `NOW()`)

√çndices:
- BTREE em `created_at`
- BTREE em `code_hash`
- GIN em `suggestions`

Recomenda√ß√µes adicionais documentadas:
- Particionamento por faixa de datas para alto volume.
- Ajuste de connection pooling (`pool_size`, `max_overflow`).
- Extens√£o `pgcrypto` habilitada pelo script para gerar UUID.

## Integra√ß√£o CrewAI

### Vis√£o Geral da Arquitetura

Este agente foi desenvolvido para ser orquestrado pela plataforma **CrewAI** conforme especificado no desafio t√©cnico. A arquitetura implementa duas camadas:

1. **An√°lise Est√°tica** (`CodeAnalyzer`): Usa `ast.parse()` para aplicar regras pr√©-definidas
2. **An√°lise com LLM** (`AdvisorCrewIntegration`): Usa CrewAI para priorizar e contextualizar as sugest√µes

### Arquitetura de Integra√ß√£o

```
Usuario ‚Üí API Endpoint ‚Üí CrewAI Workflow
                          ‚Üì
                    CodeAnalyzer (Tool)
                          ‚Üì
                    LLM (Gemini/GPT/Claude)
                          ‚Üì
                    Relat√≥rio Priorizado
```

### Como Funciona

**Endpoint com LLM (`/api/v1/analyze-code-llm`):**
- O c√≥digo √© primeiro analisado pelo `CodeAnalyzer` (an√°lise est√°tica)
- As sugest√µes s√£o enviadas para o LLM via CrewAI
- O LLM prioriza as sugest√µes, adiciona contexto e justificativas
- Retorna tanto as sugest√µes brutas quanto o relat√≥rio gerado pelo LLM

**Tool CrewAI (`analyze_python_code`):**
- `CodeAnalyzer` √© exposto como uma tool do CrewAI
- Pode ser reutilizado em outros workflows CrewAI
- Mant√©m consist√™ncia com a API REST

### Configura√ß√£o

1. Selecione o provedor via `MODEL_PROVIDER` (`openai`, `gemini`, `anthropic`, ou `azure_openai`)
2. Defina `CREWAI_API_KEY` no `.env`
3. Utilize `AdvisorCrewIntegration` para criar o agente:
   ```python
   from app.config import get_settings
   from app.crewai_integration.agent import AdvisorCrewIntegration
   from app.services.code_analyzer import CodeAnalyzer

   settings = get_settings()
   integration = AdvisorCrewIntegration(settings, CodeAnalyzer())
   agent = integration.build_agent()
   workflow = integration.build_sample_workflow()
   crew = workflow["crew"]
   result = crew.kickoff(inputs={"code_snippet": "def foo():\n    print('hi')"})
   ```

### Benef√≠cios da Integra√ß√£o

‚úÖ **An√°lise H√≠brida**: Combina an√°lise est√°tica precisa com contextualiza√ß√£o inteligente do LLM
‚úÖ **Prioriza√ß√£o Inteligente**: O LLM identifica quais problemas s√£o mais cr√≠ticos
‚úÖ **Contexto Adicionado**: Explica o "porqu√™" de cada sugest√£o
‚úÖ **Flexibilidade**: Pode ser usado como API REST ou como tool no CrewAI
‚úÖ **Escalabilidade**: F√°cil adicionar novos provedores de LLM via `ModelProviderFactory`

### Documenta√ß√£o Adicional

Consulte a documenta√ß√£o oficial da CrewAI para conectar triggers e flows empresariais: [CrewAI Docs](https://docs.crewai.com)

## Escalabilidade e Observabilidade
- **Cache**: Redis com TTL de 1h. Fallback in-memory garante disponibilidade local.
- **Filas**: Utilize RabbitMQ ou Redis Streams para enviar an√°lises volumosas a workers dedicados (ex.: Celery, RQ). O README inclui passos para evolu√ß√£o futura.
- **Horizontal Scaling**: API stateless; basta replicar inst√¢ncias atr√°s de um load balancer. Configure sticky sessions apenas se necess√°rio.
- **Banco de Dados**: Indexa√ß√£o em `code_hash` e particionamento temporal reduz leituras pesadas; considerar compress√£o do campo `code_snippet` para hist√≥ricos antigos.
- **Observabilidade**: Healthcheck consolidado, logging estruturado (ajust√°vel via `LOG_LEVEL`), espa√ßo reservado para m√©tricas em `main.py` (lifespan). Integrar com Prometheus/OpenTelemetry em etapas posteriores.

## Testes manuais sugeridos
1. **Happy path**: enviar snippet v√°lido e verificar grava√ß√£o na tabela.
2. **Cache hit**: repetir o mesmo snippet e checar `cached=true`.
3. **Erro de sintaxe**: garantir que mensagens de erro sejam retornadas.
4. **Healthcheck**: desligar Redis e verificar que o endpoint indica `cache=fallback`.
5. **CrewAI**: executar `build_sample_workflow()` e validar chamadas da tool.

---

> Refer√™ncia de documenta√ß√£o: CrewAI Docs em [docs.crewai.com](https://docs.crewai.com)
