# Advisor Code Analyzer Agent

Agente FastAPI que analisa trechos de c√≥digo Python e sugere melhorias com base em boas pr√°ticas, persistindo hist√≥rico das an√°lises em PostgreSQL, cacheando resultados em Redis e pronto para ser orquestrado via CrewAI.

> **Arquitetura SOLID**: A aplica√ß√£o foi refatorada seguindo os princ√≠pios SOLID para garantir c√≥digo modular, extens√≠vel e test√°vel. Consulte a se√ß√£o [Arquitetura](#arquitetura) para mais detalhes.

## Sum√°rio
- [Arquitetura](#arquitetura)
- [Pr√©-requisitos](#pr√©-requisitos)
- [Configura√ß√£o](#configura√ß√£o)
- [Execu√ß√£o](#execu√ß√£o)
- [Endpoints](#endpoints)
- [Base de Dados](#base-de-dados)
- [Integra√ß√£o CrewAI](#integra√ß√£o-crewai)
- [Escalabilidade e Observabilidade](#escalabilidade-e-observabilidade)
- [Extensibilidade](#extensibilidade)
- [Testes manuais sugeridos](#testes-manuais-sugeridos)

## Arquitetura

A aplica√ß√£o segue os princ√≠pios **SOLID** para garantir c√≥digo modular, extens√≠vel e manuten√≠vel.

```
app/
‚îú‚îÄ‚îÄ api/                 # Camada HTTP (routers, deps)
‚îú‚îÄ‚îÄ config.py            # Configura√ß√µes via Pydantic Settings
‚îú‚îÄ‚îÄ crewai_integration/  # Abstra√ß√£o de LLM e agente CrewAI
‚îú‚îÄ‚îÄ interfaces/          # Protocolos e contratos (SOLID DIP)
‚îú‚îÄ‚îÄ main.py              # Factory FastAPI
‚îú‚îÄ‚îÄ models/              # ORM + schemas Pydantic
‚îî‚îÄ‚îÄ services/            # Regras de neg√≥cio (analisador, cache, DB)
    ‚îú‚îÄ‚îÄ analysis_rules/  # Regras de an√°lise individuais (SRP)
    ‚îÇ   ‚îú‚îÄ‚îÄ imports.py
    ‚îÇ   ‚îú‚îÄ‚îÄ variables.py
    ‚îÇ   ‚îú‚îÄ‚îÄ functions.py
    ‚îÇ   ‚îú‚îÄ‚îÄ naming.py
    ‚îÇ   ‚îú‚îÄ‚îÄ docstrings.py
    ‚îÇ   ‚îî‚îÄ‚îÄ statements.py
    ‚îú‚îÄ‚îÄ cache/           # Backends de cache separados (SRP)
    ‚îÇ   ‚îî‚îÄ‚îÄ backends.py
    ‚îú‚îÄ‚îÄ analysis_service.py  # Orquestra√ß√£o de an√°lise
    ‚îú‚îÄ‚îÄ cache_service.py     # Facade de cache
    ‚îú‚îÄ‚îÄ code_analyzer.py     # Analisador com regras injetadas
    ‚îî‚îÄ‚îÄ database_service.py  # Servi√ßo de persist√™ncia
scripts/
‚îî‚îÄ‚îÄ init_db.sql          # Cria√ß√£o e √≠ndices da tabela analysis_history
```

### Princ√≠pios SOLID Aplicados

**Single Responsibility Principle (SRP):**
- Cada classe tem uma √∫nica responsabilidade bem definida
- Regras de an√°lise separadas (`ImportAnalysisRule`, `UnusedVariableRule`, etc.)
- Backends de cache isolados (`RedisCacheBackend`, `MemoryCacheBackend`)

**Open/Closed Principle (OCP):**
- Sistema extens√≠vel sem modificar c√≥digo existente
- Novas regras de an√°lise podem ser adicionadas facilmente
- ModelProviderFactory permite registrar novos provedores dinamicamente

**Liskov Substitution Principle (LSP):**
- Backends de cache s√£o intercambi√°veis via interface

**Interface Segregation Principle (ISP):**
- Interfaces espec√≠ficas para cada contrato (`ICacheService`, `ICodeAnalyzer`, etc.)
- Protocolos Python definem contratos claros

**Dependency Inversion Principle (DIP):**
- Depend√™ncias injetadas via FastAPI Depends
- Alto n√≠vel n√£o depende de baixo n√≠vel; ambos dependem de abstra√ß√µes
- `CodeAnalysisService` orquestra toda a l√≥gica de an√°lise

### Principais Componentes

- **CodeAnalysisService** (`app/services/analysis_service.py`): orquestra an√°lise, cache e persist√™ncia
- **CodeAnalyzer** (`app/services/code_analyzer.py`): aplica regras de an√°lise com AST; aceita regras injetadas
- **Analysis Rules** (`app/services/analysis_rules/`): regras separadas (imports, vari√°veis, fun√ß√µes, naming, docstrings, prints)
- **AnalysisHistoryService** (`app/services/database_service.py`): persiste resultados em PostgreSQL usando SQLAlchemy
- **CacheService** (`app/services/cache_service.py`): facade que gerencia Redis com fallback para cache em mem√≥ria
- **Cache Backends** (`app/services/cache/backends.py`): Redis e Memory backends isolados
- **ModelProviderFactory** (`app/crewai_integration/model_provider.py`): abstrai provedores de LLM (OpenAI, Gemini, Anthropic, Azure OpenAI)
- **AdvisorCrewIntegration** (`app/crewai_integration/agent.py`): exp√µe o agente para um workflow CrewAI

## Pr√©-requisitos
- Python 3.11+
- PostgreSQL (ou container existente)
- Docker (para subir Redis via `docker-compose`)

## Configura√ß√£o
1. Crie e ajuste o arquivo `.env` a partir do exemplo:
   ```bash
   cp .env.example .env
   ```
   - `DATABASE_URL`: string SQLAlchemy para seu PostgreSQL.
   - `REDIS_URL`: inst√¢ncia local ou remota do Redis.
   - `MODEL_PROVIDER`: `openai`, `gemini`, `anthropic` ou `azure_openai`.
   - `CREWAI_API_KEY`: chave reutilizada pelo provedor escolhido (considere vari√°veis espec√≠ficas conforme docs oficiais da CrewAI [link](https://docs.crewai.com)).
   - Opcional: `AZURE_OPENAI_ENDPOINT` e `AZURE_OPENAI_DEPLOYMENT` para uso no Azure.

2. Instale as depend√™ncias Python (idealmente em um ambiente virtual):
   ```bash
   pip install -r requirements.txt
   ```

3. Suba o Redis localmente:
   ```bash
   docker-compose up -d
   ```

4. Execute o script de cria√ß√£o da tabela (ajuste host/porta se necess√°rio):
   ```bash
   psql "$DATABASE_URL" -f scripts/init_db.sql
   ```

## Execu√ß√£o
Inicie a API com Uvicorn:
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

A aplica√ß√£o exp√µe os endpoints em `http://localhost:8000/api/v1`.

## Testes
- **Guia completo**: consulte `docs/testing.md` para pr√©-requisitos, exemplos de execu√ß√£o do `pytest` e roteiros de testes manuais (curl/Postman).
- **Execu√ß√£o r√°pida**: `python3 -m pytest` valida regras do analisador, servi√ßo de cache, camada de persist√™ncia e endpoints principais.
- **Coverage**: Todos os 8 testes passam, incluindo testes unit√°rios para regras individuais de an√°lise, backends de cache, e testes de integra√ß√£o da API.

## Endpoints
### `POST /api/v1/analyze-code`
Analisa c√≥digo Python usando an√°lise est√°tica (AST). Retorna sugest√µes t√©cnicas baseadas em regras pr√©-definidas.

**Corpo:**
```json
{
  "code": "def foo():\n    print('hello')\n",
  "language_version": "3.11"
}
```

**Resposta:**
```json
{
  "code_hash": "...sha256...",
  "suggestions": [
    {
      "rule_id": "print_statement",
      "message": "Considere utilizar logging em vez de print para sa√≠da em produ√ß√£o.",
      "severity": "info",
      "line": 2,
      "column": 4,
      "metadata": {}
    }
  ],
  "analysis_time_ms": 8,
  "cached": false
}
```

- Resultados repetidos s√£o retornados do cache Redis (campo `cached` = `true`).
- Toda an√°lise √© registrada em `analysis_history`.

### `POST /api/v1/analyze-code-llm` üÜï
Analisa c√≥digo Python usando CrewAI com LLM para gerar relat√≥rio priorizado e contextualizado. Simula a integra√ß√£o do agente com a plataforma CrewAI conforme descrito no desafio t√©cnico.

**Corpo:**
```json
{
  "code": "def foo():\n    print('hello')\n    return 1",
  "language_version": "3.11"
}
```

**Resposta:**
```json
{
  "code_hash": "...sha256...",
  "raw_suggestions": [
    {
      "rule_id": "missing_docstring",
      "message": "Fun√ß√£o 'foo' deveria conter docstring.",
      "severity": "info",
      "line": 1,
      "column": null,
      "metadata": {}
    },
    {
      "rule_id": "print_statement",
      "message": "Considere utilizar logging em vez de print para sa√≠da em produ√ß√£o.",
      "severity": "info",
      "line": 2,
      "column": 4,
      "metadata": {}
    }
  ],
  "prioritized_report": "Relat√≥rio do LLM priorizando e contextualizando as sugest√µes...",
  "model_used": "gemini",
  "analysis_time_ms": 1250,
  "cached": false
}
```

**Como Funciona:**
1. O c√≥digo √© analisado pelo `CodeAnalyzer` (an√°lise est√°tica).
2. As sugest√µes s√£o enviadas para o LLM (Gemini, GPT, Claude, etc.) via CrewAI.
3. O LLM prioriza as sugest√µes, adiciona contexto e justificativas.
4. Retorna tanto as sugest√µes brutas quanto o relat√≥rio gerado pelo LLM.

**Requisitos:**
- `CREWAI_API_KEY` configurado no `.env`.
- `MODEL_PROVIDER` definido (`openai`, `gemini`, `anthropic`, ou `azure_openai`).

**‚ö†Ô∏è Nota:** A integra√ß√£o com CrewAI/LLM requer configura√ß√£o adicional conforme documenta√ß√£o oficial do CrewAI. O endpoint `/api/v1/analyze-code` (an√°lise est√°tica) funciona sem depend√™ncias adicionais de LLM.

### `GET /api/v1/health`
Retorna status das depend√™ncias (`database`, `cache`, `model_provider`).

## Base de Dados
Tabela `analysis_history`:
- `id` (UUID, gerado automaticamente)
- `code_hash` (hash SHA-256 do snippet)
- `code_snippet` (texto bruto, opcional)
- `suggestions` (JSONB com lista de recomenda√ß√µes)
- `analysis_time_ms`, `language_version`
- `created_at` (`TIMESTAMPTZ`, default `NOW()`)

√çndices:
- BTREE em `created_at`
- BTREE em `code_hash`
- GIN em `suggestions`

Recomenda√ß√µes adicionais documentadas:
- Particionamento por faixa de datas para alto volume.
- Ajuste de connection pooling (`pool_size`, `max_overflow`).
- Extens√£o `pgcrypto` habilitada pelo script para gerar UUID.

## Integra√ß√£o CrewAI

### Vis√£o Geral da Arquitetura

Este agente foi desenvolvido para ser orquestrado pela plataforma **CrewAI** conforme especificado no desafio t√©cnico. A arquitetura implementa duas camadas:

1. **An√°lise Est√°tica** (`CodeAnalyzer`): Usa `ast.parse()` para aplicar regras pr√©-definidas
2. **An√°lise com LLM** (`AdvisorCrewIntegration`): Usa CrewAI para priorizar e contextualizar as sugest√µes

### Arquitetura de Integra√ß√£o

```
Usuario ‚Üí API Endpoint ‚Üí CrewAI Workflow
                          ‚Üì
                    CodeAnalyzer (Tool)
                          ‚Üì
                    LLM (Gemini/GPT/Claude)
                          ‚Üì
                    Relat√≥rio Priorizado
```

### Como Funciona

**Endpoint com LLM (`/api/v1/analyze-code-llm`):**
- O c√≥digo √© primeiro analisado pelo `CodeAnalyzer` (an√°lise est√°tica)
- As sugest√µes s√£o enviadas para o LLM via CrewAI
- O LLM prioriza as sugest√µes, adiciona contexto e justificativas
- Retorna tanto as sugest√µes brutas quanto o relat√≥rio gerado pelo LLM

**Tool CrewAI (`analyze_python_code`):**
- `CodeAnalyzer` √© exposto como uma tool do CrewAI
- Pode ser reutilizado em outros workflows CrewAI
- Mant√©m consist√™ncia com a API REST

### Configura√ß√£o

1. Selecione o provedor via `MODEL_PROVIDER` (`openai`, `gemini`, `anthropic`, ou `azure_openai`)
2. Defina `CREWAI_API_KEY` no `.env`
3. Utilize `AdvisorCrewIntegration` para criar o agente:
   ```python
   from app.config import get_settings
   from app.crewai_integration.agent import AdvisorCrewIntegration
   from app.services.code_analyzer import CodeAnalyzer

   settings = get_settings()
   integration = AdvisorCrewIntegration(settings, CodeAnalyzer())
   agent = integration.build_agent()
   workflow = integration.build_sample_workflow()
   crew = workflow["crew"]
   result = crew.kickoff(inputs={"code_snippet": "def foo():\n    print('hi')"})
   ```

### Benef√≠cios da Integra√ß√£o

‚úÖ **An√°lise H√≠brida**: Combina an√°lise est√°tica precisa com contextualiza√ß√£o inteligente do LLM
‚úÖ **Prioriza√ß√£o Inteligente**: O LLM identifica quais problemas s√£o mais cr√≠ticos
‚úÖ **Contexto Adicionado**: Explica o "porqu√™" de cada sugest√£o
‚úÖ **Flexibilidade**: Pode ser usado como API REST ou como tool no CrewAI
‚úÖ **Escalabilidade**: F√°cil adicionar novos provedores de LLM via `ModelProviderFactory`

### Documenta√ß√£o Adicional

Consulte a documenta√ß√£o oficial da CrewAI para conectar triggers e flows empresariais: [CrewAI Docs](https://docs.crewai.com)

## Escalabilidade e Observabilidade
- **Cache**: Redis com TTL de 1h. Fallback in-memory garante disponibilidade local.
- **Filas**: Utilize RabbitMQ ou Redis Streams para enviar an√°lises volumosas a workers dedicados (ex.: Celery, RQ). O README inclui passos para evolu√ß√£o futura.
- **Horizontal Scaling**: API stateless; basta replicar inst√¢ncias atr√°s de um load balancer. Configure sticky sessions apenas se necess√°rio.
- **Banco de Dados**: Indexa√ß√£o em `code_hash` e particionamento temporal reduz leituras pesadas; considerar compress√£o do campo `code_snippet` para hist√≥ricos antigos.
- **Observabilidade**: Healthcheck consolidado, logging estruturado (ajust√°vel via `LOG_LEVEL`), espa√ßo reservado para m√©tricas em `main.py` (lifespan). Integrar com Prometheus/OpenTelemetry em etapas posteriores.

## Extensibilidade

A arquitetura SOLID facilita a extens√£o do sistema:

### Adicionar Novas Regras de An√°lise

Crie uma nova regra estendendo `BaseAnalysisRule`:

```python
from app.services.analysis_rules.base import BaseAnalysisRule
from typing import Any, Dict, List
import ast

class MyCustomRule(BaseAnalysisRule):
    rule_id = "my_custom_rule"
    
    def _analyze(self, tree: ast.AST, suggestions: List[Dict[str, Any]]) -> None:
        # Sua l√≥gica de an√°lise aqui
        pass
```

Em seguida, registre a regra no `CodeAnalyzer`:

```python
from app.services.code_analyzer import CodeAnalyzer
from app.services.analysis_rules import ImportAnalysisRule, UnusedVariableRule
from my_module import MyCustomRule

rules = [
    ImportAnalysisRule(),
    UnusedVariableRule(),
    MyCustomRule(),  # Nova regra
]
analyzer = CodeAnalyzer(rules=rules)
```

### Adicionar Novo Provedor de LLM

Registre um novo provedor de modelo dinamicamente:

```python
from app.crewai_integration.model_provider import ModelProviderFactory, BaseModelProvider

class MyCustomProvider(BaseModelProvider):
    provider_name = "my_provider"
    
    def get_llm_config(self):
        return {"model": "my-model", "api_key": self.api_key}

ModelProviderFactory.register_provider("my_provider", MyCustomProvider)
```

### Adicionar Novo Backend de Cache

Crie um novo backend implementando `ICacheBackend`:

```python
from app.services.cache.backends import ICacheBackend
from typing import Any, Optional

class MyCacheBackend(ICacheBackend):
    def get(self, key: str) -> Optional[Any]:
        # Sua implementa√ß√£o
        pass
    
    def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None) -> None:
        # Sua implementa√ß√£o
        pass
    
    def is_available(self) -> bool:
        return True
```

Em seguida, use-o no `CacheService`:

```python
from app.services.cache_service import CacheService
from my_module import MyCacheBackend

my_backend = MyCacheBackend()
cache = CacheService("", primary_backend=my_backend)
```

## Testes manuais sugeridos
1. **Happy path**: enviar snippet v√°lido e verificar grava√ß√£o na tabela.
2. **Cache hit**: repetir o mesmo snippet e checar `cached=true`.
3. **Erro de sintaxe**: garantir que mensagens de erro sejam retornadas.
4. **Healthcheck**: desligar Redis e verificar que o endpoint indica `cache=fallback`.
5. **CrewAI**: executar `build_sample_workflow()` e validar chamadas da tool.

---

> Refer√™ncia de documenta√ß√£o: CrewAI Docs em [docs.crewai.com](https://docs.crewai.com)
